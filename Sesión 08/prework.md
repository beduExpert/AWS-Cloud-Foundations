# Prework

# Introducci√≥n
Hay varios t√≥picos en AWS m√°s avanzados, los anteriores fueron los bloque fundamentales que toda la nube de AWS est√° construida. 
T√≥picos avanzados de AWS se construyen y manejan sobre los t√≥picos b√°sicos, se dar√° por hecho que los t√≥picos b√°sicos se conocen por lo que no se profundizar√° en ellos al ser mencionados.


# 1. Objetivo üéØ
- Conocer los mecanismos para comunicaci√≥n b√°sica entre servicios de AWS.
- Conocer los mecanismos para monitoreo, configuraci√≥n y cumplimiento de normas y est√°ndares.

# 2. Instrucciones üìã
- Se debe leer el contenido en un ambiente libre de distractores f√≠sicos y electr√≥nicos atendiendo a los enlaces en el texto.

# 3. Desarrollo üìë

# App Integration Services

- Amazon SQS:
* Servicio de colas de mensajes especialmente √∫til en sistemas distribuidos, de microservicios y serverless totalmente administrado, no hay que preocuparse por mantener servidores y recursos para que funcione, escala de forma autom√°tica pasando de 1 mensaje a millones de mensajes sin esfuerzo. La caracter√≠stica principal brindada a las aplicaciones desarrolladas es el desacople entre componentes y manejo de tareas as√≠ncronas, precisamente el hecho de asincronismo es la caracter√≠stica principal de SQS ya que los clientes deben estar preguntando a SQS constante por la llegada de nuevos mensajes para procesar en un modelo de sondeo.
El principio de funcionamiento es sencillo, se ven involucrados solo tres actores, el primero es el _productor_; es encargado de generar informaci√≥n, el siguiente es la _cola de mensaje_ en ella se guardan los mensajes hasta que un _consumidor_ se conecte a la cola para recuperar los mensajes y procesarlos.
Los mensajes se basan en dos modelos, el modelo FIFO y el modelo est√°ndar, en el primero se garantiza que as√≠ como llegan los mensajes a SQS en ese orden saldr√°n (First Input First Output), mientras en el segundo no se garantiza.
![prework-aws-sqs-fifo-estandard-01.png](prework-aws-sqs-fifo-estandard-01.png)


¬øEn qu√© casos de uso es √∫til?
Un sistema de e-commerce puede manejar millones de transacciones en temporadas como el buen fin, se pueden ir guardando todas las ordenes de compra de los clientes en SQS para ir siendo procesadas pro m√∫ltiples "Order Processors" o _workers_ sin que se sienta como un cuello de botella al cliente final ya que SQS escala autom√°ticamente.
![prework-order-service-01.png](prework-order-service-01.png)
Se recomienda leer el siguiente [art√≠culo](https://aws.amazon.com/es/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/) para profundizar m√°s.

Se integra con otros servicios como DynamoDB, RDS, ECS, Redshift, EC2, S3 y Lambda, especialmente interesante la integraci√≥n con  Lambda para el procesado de mensajes de forma Serverless, recordar que Lambda y API Gateway escalan autom√°ticamente por lo que no habr√≠a en principio cuellos de botella en la aplicaci√≥n.
El esquema de cobro es basado en el tama√±o del mensaje que SQS debe procesar, y las acciones de sondeo, borrado y alta de un mensaje. Las colas de tipo FIFO  y est√°ndar tienen distintos precios, FIFO tiene un precio m√°s alto por el procesamiento extra para garantizar el orden de los mensajes.

- Amazon SNS
* Es el servicio dise√±ado parara comunicar Aplicaciones y personas mediante el env√≠o de mensajes SMS, push notificaciones y email, tambi√©n Aplicaci√≥n a Aplicaci√≥n por medio del modelo [pub-sub](https://hackernoon.com/publish-subscribe-design-pattern-introduction-to-scalable-messaging-781k3tae) (publicador-suscriptor) hacia SQS,  Lambdas o webhooks con http/https. Al igual que SQS el principal beneficio de SNS es el desacople de aplicaciones, pero a diferencia SNS los clientes no tienen que sondear por mensajes ya que SNS hace push de los mensajes en uno o varios canales (a.k.a temas), los clientes se suscriben a los canales necesarios y comienzan a recibir los mensajes. SNS va bien con aplicaciones que tienen que ver con monitoreo, notificaciones de eventos como cambio de inventarios, baja de productos, detecci√≥n de anomal√≠as sobre un producto, apps m√≥viles. Para usarlo basta con generar un "tema" (se conoce igual como canal o t√≥pico) se puede ver como una sala de chat, despu√©s los clientes se pueden suscribir a este tema para recibir los mensajes (al igual que en un chat dependiendo del tema de inter√©s una persona se suscribir√° al canal para recibir todos los mensajes relacionados a √©l). La naturaleza de SNS es s√≠ncrona, por lo que no hay necesidad de hacer sondeos peri√≥dicos por parte de los clientes. Una forma conocida y usada de SQS con SNS es usar SNS como receptor de un mensajes con capacidad de entregar mensajes a m√∫ltiples colas SQS para ir siendo procesadas por estas al ritmo que cada una requiera. En cuanto a la facturaci√≥n depender√° de los clientes que se suscriban a los temas o canales, es diferente la entrega de mensajes por medio de email que por SMS. 

El siguiente es un ejemplo como un sistema de historia cl√≠nica (Electronic Medical Record) usa SNS y SQS en conjunto para despachar acciones que genera el m√©dico al interactuar con el sistema hacia otros subsistemas o microservicios, en este caso son el servicio de cuentas (Billing), el de Prescripci√≥n m√©dica y el sistema de informaci√≥n y recordatorios sobre citas y recordatorios (Scheduling).
![prework-sistema-clinico-01.png](prework-sistema-clinico-01.png)


- Amazon MQ:
Con el advenimiento de aplicaciones cada vez mas complejas el modelo monol√≠tico ha dejado poco a poco de tener relevancia en la industria, por lo menos en aplicaciones que exigen alta transaccionalidad (decenas de millones de peticiones al d√≠a), aqu√≠ surgen a la luz las arquitecturas basadas en eventos (Event Driven Architecture EDA). EDA es una forma de arquitectar aplicaciones de forma muy distinta al monolito, en EDA la aplicaci√≥n es fragmentada en distintos "servicios", cada uno especializado en hacer una cosa, cada uno de estos servicios cuenta con su propia base de datos, su propia interfaz y protocolo de comunicaci√≥n y sus propias reglas de negocio, incluso puede cada uno estar hecho en lenguajes de programaci√≥n distintos. Estas caracter√≠sticas obligan a implementar mecanismos de comunicaci√≥n eficientes entre cada servicio, un error com√∫n es querer comunicar servicios de forma encadenada, donde el servicio A llama al servicio B y el servicio B llama al servicio C, en este caso si el servicio C falla, A y B lo har√°n tambi√©n, por lo que la forma de comunicarse debe cambiar, normalmente es hacerlo por medio de un bus de comunicaci√≥n, un canal al que todos los servicios est√°n comunicados ya sea mandando mensajes dirigidos a servicios espec√≠ficos o recibi√©ndolos para procesarlos. El bus es el eje central de las arquitecturas EDA por lo que tener un bus fiable se vuelve cr√≠tico, una de las caracter√≠sticas de los microservicios es que si uno falla o tiene un rendimiento degradado los otros servicios no tienen que verse afectados, pero si el bus falla toda la aplicaci√≥n fallar√≠a con el riesgo incluso de inconsistencia de datos.
Para tal uso AWS ofrece un bus de mensajes totalmente administrado, con alta disponibilidad compatible con los buses de mensajer√≠a populares en la industria como RabbitMQ y ActiveMQ.
Imaginar un entornos con un aplicativo de ventas en fechas de navidad, el tr√°fico ser√° muy alto, seguramente el servicio que da vida a la interfaz de la aplicaci√≥n se ver√° altamente saturado, en menor medida el sistema de pedidos ya que por cada diez visitas y b√∫squedas solo cinco se concretan a una venta. La interfaz gr√°fica donde se hacen los pedidos ser√° el _productor_ (producer) de mensajes (cada mensaje es una orden de compra) y el sistema de pedidos e inventarios ser√°n consumidores, tomar√°n las ordenes de compra y cada uno ejecutar√° las tareas para las que fue dise√±ado para cada orden de compra.
![prework-sistema-de-ventas-01.png](prework-sistema-de-ventas-01.png)

[http://tryrabbitmq.com](http://tryrabbitmq.com)



- Amazon SWF (Amazon Simple Workflow):
SWF es un orquestador o manejador de flujo de tareas entre servicios de aplicaciones distribuidas. SWF puede ayudar a acelerar el desarrollo de aplicaciones en las etapas donde se requiere guardar un estado de la aplicaci√≥n es decir saber cuales tareas se han completado y cuales siguen su curso siendo no necesario implementar bases de datos y software para el monitoreo, ¬øqu√© sucede con la comunicaci√≥n de tareas? algunos procesos de negocio requieren que algunas tareas se ejecuten antes que otras por lo que las tareas deben estar al tanto de lo que hacen las otras, el desarrollador se puede ahorrar el desarrollo de canales de comunicaci√≥n sin comprometer la integridad de la informaci√≥n, otra ventaja es contar con esta l√≥gica de flujo centralizada en un solo lugar con las ventajas de mantenimiento que esto trae. Siguiendo con el seguimiento de estado, algunas aplicaciones requieren en su flujo de operaci√≥n la aprobaci√≥n antes de pasar al siguiente flujo, imaginar un sistema de pagos, se generan varias ordenes de pago a proveedores y al final del d√≠a el director financiero debe revisarlas y aprobarlas todas, con SWF este estado de "pendientes de aprobar" queda latente hasta que el director financiero expl√≠citamente en la aplicaci√≥n las pase a estado "aprobado".
Se tienen por un lado los _workers_ y los _deciders_, los primeros se encargan de la ejecuci√≥n de las tareas y devoluci√≥n de resultados una ves ejecutada, se pueden ejecutar sobre instancias EC2 o Lambdas, los _deciders_ coordinan la l√≥gica de ejecuci√≥n definiendo el paso a paso en el procesamiento, por ejemplo, se pueden reintentar tareas en caso de fallas, omitir tareas dada una condici√≥n espec√≠fica por parte de un worker con lo que se puede cambiar el flujo de la aplicaci√≥n f√°cilmente. Una gran caracter√≠stica de SWF es el control por medio de su propio SDK de desarrollo, el llamado AWS Flow Framework, con √©l es posible generar flujos complejos de coordinaci√≥n de tareas (deciders). Al generar un flujo con el SDK el programa se comunica con SWF para ejecutar los flujos correspondientes en el tiempo preciso.

- AWS Step Functions:
Step functions es un orquestador de tareas para aplicaciones distribuidas generalmente basadas en microservicios. Step Funcions permite de modo visual y/o gr√°fico ordenar y visualizar los servicios de las aplicaciones en una serie de pasos ordenados en base a los flujos que el negocio requiera. 
Step Funcions en realidad es la implementaci√≥n comercial de una [_m√°quina de estados_](https://whatis.techtarget.com/definition/state-machine), cada estado tiene una serie de instrucciones definidas en base a par√°metros espec√≠ficos. La desventaja de Step Funcions ante SWF es que es mas limitado en los casos de uso soportados, Step Funcions se limita a definir el comportamiento de la m√°quina de estados a un archivo JSON, por lo que modelar flujos complejos y mantenerlos puede llegar a ser una tarea retadora. Ante flujos complejos se recomienda el uso de AWS SWF.

![prework-ejemplo-flujo-visual-demaquina-estados-01.png](prework-ejemplo-flujo-visual-demaquina-estados-01.png)


- Amazon AppFlow:
AppFlow permite la integraci√≥n de SaaS de la industria como Salesforce, Datadog, Slack, Zendesk o ServiceNow con servicios de AWS como S3 o RedShift o incluso con otros SaaS del mercado sin necesidad de contar con personal altamente calificado y esperar meses de desarrollo antes de ver resultados. 
Se basa en el principio de mapeo de informaci√≥n por flujos. Primero habr√° que definir un flujo en el que se dicta cu√°l es el software as a service que se desea conectar, posteriormente se define el destino de la informaci√≥n que ser√° extra√≠da como puede ser un bucket de S3, luego habr√° que definir un mapeo entre los datos de origen y destino ya que cada SaaS maneja diferentes esquemas de datos, definido eso se debe especificar si la tarea ser√° una tarea recurrente o bajo demanda, posteriormente se debe especificar bajo que condiciones la informaci√≥n debe ser transferida, por ejemplo se podr√≠a configurar solo transferir informaci√≥n de Salesforce hacia un bucket de S3 con todas las ventas mayores a 100,000 USD, o especificar que los datos de una determinada campa√±a de Google Analytics se transfieran a [UpSolver](https://www.upsolver.com) para su procesamiento.



- Amazon EventBridge:
EventBridge se puede definir como Serverless Event Bus, se especializa en la escucha de eventos que suceden en los propios servicios de AWS (m√°s de 90) o de terceros, los eventos son por ejemplo el cambio de estado de una instancia EC2, este evento puede ser puesto a disposici√≥n de otros servicios por medio de un bus por defecto del servicio o uno hecho a medida hacia el servicio AWS SNS ser√≠a posible definir el env√≠o de un mensaje SMS a un n√∫mero dado si es que alguna instancia es encendida por ejemplo.
Quien define este comportamiento son las reglas de ejecuci√≥n, en ellas se especifica el evento y por ende el servicio que detonar√° una regla, se debe especificar si esta regla se debe ejecutar en una periodicidad dada o responda a un evento, despu√©s se deber√° escoger el tipo de evento que se desea escuchar por ejemplo si se selecciona como fuente un bucket S3 solo se podr√≠a especificar la  escucha de eventos PutObject.

[https://aws.amazon.com/es/blogs/mt/bbva-automated-responses-through-event-management-scale/?nc1=b_rp](https://aws.amazon.com/es/blogs/mt/bbva-automated-responses-through-event-management-scale/?nc1=b_rp)





# AWS Messaging Services
- Amazon Pinpoint:
La herramienta de AWS para marketing y engagement de clientes por medio los canales de mensajes SMS, llamadas de voz, email y push notifications. Es compatible con otros servicios de AWS, en general es posible trabajar _campa√±as_ para uno o varios _segmentos_ de  usuarios, se pueden definir mensajes predeterminados o mensajes basados en atributos para dar una sensaci√≥n de mensajes personalizados a los clientes.
El env√≠o de mensajes seg√∫n el segmento permitir√° hacer exclusi√≥n de clientes, determinar si el env√≠o es de una √∫nica vez o peri√≥dicamente, se puede programar la hora de env√≠o de mensajes incluso se pueden enviar mensajes seg√∫n ciertas acciones del usuario, finalmente ser√° posible analizar el comportamiento de los usuarios por ejemplo saber si los emails enviados fueron abiertos y en cuanto tiempo a partir del env√≠o.

- Amazon Kinesis
Streaming data son datos generados continuamente "sin fin" por cientos de fuentes que pueden ser utilizados aun sin necesidad de ser descargados primero. Se puede ver como el agua que fluye en un r√≠o, de forma similar, los datos son generados por varios tipos de fuentes en formatos diversos y vol√∫menes distintos, desde aplicaciones, dispositivos de red, dispositivos IoT, transacciones en sitios web, datos de ubicaci√≥n, etc. Por ejemplo, cuando un usuario de servicios de transporte privado llama un servicio se genera un stream de datos proveyendo la localizaci√≥n del usuario, por otro lado se debe juntar el stream de datos sobre el tr√°fico, con ellos se debe poder calcular el precio a cobrar todo en tiempo real. Ese solo fue un ejemplo, los casos de uso t√≠picos son actualizaci√≥n de inventarios, forecasting, monitoreo de logs, actividad de los usuarios, detecci√≥n de fraude, datos de localizaci√≥n, pool services o servicios en coincidencia (como car pool) combinando localizaci√≥n y presupuestos de los usuarios basados en proximidad, destino y precios. 
Kinesis es el servicio de alta disponibilidad con soporte para manejo de mensajes bajo la arquitectura _producer_ y _consumer_.
Kinesis se subdivide en servicios especializados de acuerdo a necesidades espec√≠ficas,  Kinesis Data Streams es m√°s acorde para el desarrollo de aplicaciones de streaming de necesidades espec√≠ficas, incluso el provisionamiento de capacidad de manejo de mensajes es controlado por el administrador, tiene capacidad de retenci√≥n de datos de hasta siete d√≠as, pr√°cticamente dise√±ado en tiempo real. Kinesis Data Firehose es el servicio listo para ingesta de datos en streaming deposit√°ndolos directamente en un lago de datos como S3 o Redshift, indexado de informaci√≥n como Amazon Elasticsearch Service o incluso puntos de enlace http, proveedores como New Relic y Mongo DB tambi√©n son soportados. Es un servicio totalmente administrado aunque no cuenta con retenci√≥n de datos, el valor agregado de Firehose es que se pueden modificar o preparar los datos antes de ser cargados en el data lake, aunque esto puede impactar un poco en el performance, Kinesis Data Analytics permite el an√°lisis de datos en tiempo real sobre un stream de datos, se evita esperar horas o d√≠as antes de ser procesada la informaci√≥n, en su lugar deber√°n ser solo segundos o minutos. 
Por √∫ltimo Kinesis Video Streams dise√±ado para la transmisi√≥n de v√≠deo en vivo no solo a otras personas, tambi√©n a modelos de machine learning para an√°lisis, en tiempos de pandemia se vuelve interesante transmitir v√≠deo para detectar zonas o puntos rojos donde no se usan mascarillas faciales, al final esos datos se pueden correlacionar para formar mapas de calor par establecer un cerco sanitario.

- Agente de mensajes de AWS IoT:
Los agentes de mensajes permiten la transmisi√≥n de desde y hacia dispositivos IoT con soporte para protocolos MQTT y WebSockets. 


# AWS Config: seguridad reactiva

Especializado en auditor√≠a y compliance, de ah√≠ el t√©rmino seguridad reactiva, b√°sicamente AWS config permite poner en un dashboard todos los servicios utilizados en todas las regiones con el estado de compliance que se se defina en _reglas_ de trabajo. Por ejemplo, es posible definir el escaneo de todas las instancias de EC2 y comprobar que dichas instancias tengan solo el puerto 22 abierto a direcciones IP espec√≠ficas, de no cumplir con esta regla habr√° una alarma en el dashboard.
Hay una forma m√°s eficiente de hacer auditor√≠a de buenas pr√°cticas dependiendo la industria o necesidades, es por medio de  _paquete de conformidad_  (Conformance Pack) , hay una serie de plantillas pre definidas, cada paquete esta compuesta de una regla, lo que hace muy eficiente a la hora de auditar, un ejemplo es el paquete **# Operational Best Practices for PCI DSS 3.2.1** que ayudar√° en el cumplimiento re las reglas que tienen que ver con el manejo de informaci√≥n de tarjetas de cr√©dito, cuenta con unas 70 reglas a cumplir. 
Actualmente se cuentan con m√°s de 65 paquetes de reglas listos para ser utilizados.
AWS Config se vuelve muy atractivo a la hora de auditar m√∫ltiples cuentas de AWS, es posible por medio de un _agregador_ concentrar los datos de otros servicios AWS Config de otras cuentas.
No es la herramienta m√°s atractiva visualmente, pero la facilidad de integraci√≥n es indiscutible, aunque esto es una de sus mayores desventajas pues solo soporta servicios de AWS, en caso de tener un esquema multicloud habr√° que usar herramientas como Splunk o SolarWinds.
Vale la pena usar la herramienta, el hecho de poder tener un inventario de todo lo utilizado que ya es muy bueno se contar√° con capacidades para guardar todos los cambios de configuraciones que se hagan sobre los recursos, con esa informaci√≥n se puede reducir considerablemente el tiempo de resoluci√≥n de fallas.




# AWS Systems Manager
System manager es una herramienta para gesti√≥n de infraestructura no limitado solo a recursos de AWS, se puede usar para la gesti√≥n de servidores on-premise f√≠sicos o virtuales.
Se basa en cuatro pilares; monitoreo, auditor√≠a, optimizaci√≥n y ejecuci√≥n. 

System manager se subdivide en:
- Administraci√≥n de aplicaciones
* Grupos de recursos: Es una forma de organizaci√≥n de recursos de AWS haciendo f√°cil la administraci√≥n de ellos, especialmente √∫til cuando hay una larga lista de recursos a administrar
* AppConfig: Tiene la capacidad de crear y manejar el despliegue de configuraciones de aplicaci√≥n, puede ser usado para encender o apagar caracter√≠sticas de aplicaciones como un anuncio, otro caso de uso es permitir a usuarios de paga acceso a contenido exclusivo.
* Parameter Store: Provee un repositorio seguro de secretos, es posible guardar de forma segura passwords, cadenas de conexi√≥n de base de datos, c√≥digos de licencias, etc. Es especialmente √∫til para incrustar contrase√±as o informaci√≥n sensible en scripts, comandos, documentos de system manager y flujos de automatizaci√≥n.

- Administraci√≥n de operaciones
* Explorador: Es un dashboard que reporta informaci√≥n sobre los recursos de AWS, normalmente incluyendo metadata de instancias EC2.
* OpsCenter: Permite al personal de operaciones manejar incidentes con la ayuda de m√©tricas como utilizaci√≥n de CPU de las instancias EC2, cargos estimados de facturas, status check de instancias, espacio en discos EBS.
* Panel de CloudWatch: Son dashboards configurables que pueden ser usados para monitorear los recursos de AWS en una √∫nica vista a√∫n siendo recursos en diferentes regiones.
* Personal Health Dashboard: Provee informaci√≥n sobre la salud de los servicios de AWS, la informaci√≥n se presenta en eventos  programados y en un hist√≥rico de eventos de los √∫ltimos 90 d√≠as.

- Acciones y cambios
* Automatizaci√≥n: Simplifica las tareas de mantenimiento comunes o repetitivas de algunos recursos de AWS entre ellos instancias EC2, permite el manejo de flujos de trabajo por medio de documentos json o yml. Las tareas de automatizaci√≥n pueden ser tan sencillas como apagar instancias, pero al hablar de decenas de instancias separadas por regiones esa simple tarea puede llevar mucho esfuerzo. 
* Cambiar calendario: Se pueden programar tareas de automatizaci√≥n, ¬øse requiere un cambio a media madrugada?.
* Periodos de mantenimiento:  Permite definir tareas programadas que pueden potencialmente interrumpir las operaciones como lo son parches de seguridad sobre el sistema operativo o sobre software de aplicativos.

- Instancias y nodos: Provee las siguientes acciones en instancias EC2 o en servidores locales ya sea f√≠sicos o  virtuales, adem√°s de recursos de AWS.
* Conformidad: Usado para escaneo y comprobaci√≥n de cumplimiento de parches de seguridad en instancias.
* Inventario: Provee la visibilidad necesaria de la recolecci√≥n de datos de instancias EC2 o servidores locales. Los datos recolectados son guardados en un bucket S3 y despu√©s ser explotados para conocer que instancias est√°n ejecut√°ndose, que instancias requieren alg√∫n tipo de actualizaci√≥n por ejemplo. 
* Instancias administradas: Ver y administrar instancias centralizadamente ya sea EC2 o instancias locales incluyendo sistemas operativos Windows, Linux e incluso dispositivos Raspberry Pi. 
* Activaciones h√≠bridas: Es el panel donde se pueden dar de alta recursos externos a AWS, el panel provee un mecanismo de autorizaci√≥n para poder agregar un recurso a AWS System manager de forma segura.
* Session Manager: Permite la conexi√≥n a instancias EC2 o servidores locales f√≠sicos o virtuales por medio de una consola de l√≠nea de comandos en una ventana web sin necesidad de abrir puertos, tener servidores de administraci√≥n adicionales ni manejo de llaves SSH. Session manager ayuda al cumplimiento de pol√≠ticas de seguridad corporativas relativas al acceso a recursos.
* Run Command: Permite remotamente manejar la configuraci√≥n de instancias EC2 o servidores locales f√≠sicos o virtuales. Se pueden ejecutar comando relacionados con la construcci√≥n de flujos de despliegue de aplicaciones, captura de logs, uni√≥n de servidores a un dominio de Windows por ejemplo.
* State Manager: Permite establecer configuraciones espec√≠ficas para instancias EC2 o servidores externos a AWS, las configuraciones son el estado que se desea mantener, una definici√≥n de estado puede establecer que se debe instalar un software espec√≠fico y adem√°s ciertos puertos deben ser cerrados o abiertos.
* Patch Manager: Automatiza el proceso de mantener las instancias con las √∫ltimos parches de seguridad, soporta aplicaci√≥n de parches para Windows, AWS Linux, CentOS, Debian, Red Hat, SUSE Linux y Ubuntu Server.
* Distribuidor: Permite empacar software por ejemplo antivirus, para instalar en instancias manejadas por System Manager. 

- Documentos: Bajo el contexto System Manager un documento (document) es una secuencia de acciones a seguir ya sea en YAML o JSON, con ello se reduce el error humano. Los documentos soportan versionado, se pueden tener documentos de meses anteriores disponibles para usarse en el momento que se requieran. System Manager incluye mas de 100 documentos preconfigurados clasificados en  Command document usados para ejecutar comandos y aplicar configuraciones sobre instancias, Automation document usado para ejecutar tareas de mantenimiento y despliegue, Policy document obligan al seguimiento de pol√≠ticas de seguridad por √∫ltimo Session document para determinar una sesi√≥n de conexi√≥n por un t√∫nel ssh o redirecci√≥n de puertos.

# AWS Organizations y Control Tower
AWS organizations es con servicio para la gesti√≥n de m√∫ltiples cuentas de AWS.
La gesti√≥n consiste en manejar la facturaci√≥n, control de accesos, seguridad y compliance.
Para algunas compa√±√≠as una estructura multi cuenta ayuda a cumplir con ciertos objetivos estrat√©gicos. El primero y m√°s obvio es el aislamiento de redes, pr√°cticamente los servicios de una cuenta no degradan a o se ven afectados por los servicios de otra, modularidad, escalabilidad y sobre todo compliance limitando especialmente se agradece a la hora de las auditor√≠as, se pueden usar los servicios e infraestructura que requieren alguna certificaci√≥n como PCI DSS en una cuenta √∫nica, para lo dem√°s se usar√≠a otra.
Con Organizations se pueden las cuentas guardar en grupos (unidades Organizacionales o UO), sobre esos grupos se pueden aplicar pol√≠ticas espec√≠ficas.
B√°sicamente Organizations maneja se maneja con una cuenta maestra, es la cuenta que construye la organizaci√≥n (una vez establecida no puede ser cambiada), desde esta cuenta se generan otras cuentas en la organizaci√≥n o se remueven cuentas de la organizaci√≥n.  

Control Tower es un servicio que automatiza el proceso de creaci√≥n de una l√≠nea base para entornos multi cuenta que son seguros y que normalmente cumplen el well architected framework. Es interesante que el servicio ayuda a incorporar el conocimiento y la experiencia que se va ganando. Control Tower se asienta en los hombros de otros servicios como AWS Organizations, AWS IAM, AWS Config y AWS Cloud Trail.
Para entender Control Tower conviene profundizar en los elementos que lo componen:


Landing Zone: el entorno general de m√∫ltiples cuentas que Control Tower configura.

Blueprints: patrones de dise√±o bien dise√±ados que se utilizan para configurar la Landing Zone.

Guardrails: implementaciones automatizadas de controles de pol√≠ticas, con un enfoque en la seguridad, el cumplimiento y la administraci√≥n de costos.

Environment : una cuenta de AWS y los recursos que contiene, configurados para ejecutar una aplicaci√≥n.


# Integraci√≥n de datos On-premise
- AWS Data Pipeline: 
Es el servicio para automatizar la transformaci√≥n y movimiento de datos. Tiene el connotativo _pipeline_ por que se pueden establecer flujos de trabajo dependientes de si etapas previas fueron fallidas o exitosas.
Se compone de tres grande componentes, la definici√≥n del flujo, las fechas o periodos de ejecuci√≥n y los ejecutores de tareas que pueden ser instancias EC2, instancias manejadas por Data Pipeline y servidores on premise.

Pileline se integra con los servicios DynamoDB, RDS, Redshift y S3, para la etapa de servicios de c√≥mputo se integra con EC2 y clusters EMR.

La etapa de definici√≥n es importante para establecer los formatos que ser√°n soportados (json, csv, etc), las actividades que transformaran de los datos, las precondiciones que deben ser satisfechas antes de que las actividades puedan ser programadas
Los nodos de datos (Data nodes) representan los tipos de datos y la localizaci√≥n que puede ser accedido por Pipeline, se incluyen elementos de salida y entrada.
Las Actividades (Activities) representan acciones en el flujo de trabajo, CopyActivitiy, SQLActivity, ShellCommandActivity son solo algunos tipos de actividades soportadas.
Recursos (Resuources) son las instancias EC2 o cluster EMR usado.

- AWS Glue:
Ya se ha hablado en el pasado sobre Glue, el servicio de ETL administrado de AWS, tiene una caracter√≠stica interesante, se puede conectar a repositorios de datos locales por medio de Java‚Ñ¢ Database Connectivity (JDBC), la especificaci√≥n de conexi√≥n estandard para la conexi√≥n y la gesti√≥n de bases de datos. Coursera usa a AWS Glu para integrar los datos de distintas fuentes de datos a un warehouse en Redshift para despu√©s poder generar el sistema de recomendaciones. Como referencia consultar [aqu√≠](https://aws.amazon.com/es/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/).


# Procesamiento, an√°lisis de datos y machine learning

AWS proporciona varias capas para poder cubrir la cuota de mercado referente a machine learning. En el nivel mas alto se tiene los servicios de AI, se caracterizan por por que no es necesario una habilidad t√©cnica especial, es del tipo de servicios solo consuma y disfrute. 

En el siguiente nivel se tienen los denominados ML services, son servicios que requieren ciertas habilidades t√©cnicas sobre el tema como hyperparametrizaci√≥n y selecci√≥n de features, aunque los servicios de esta capa proveen lo necesario para facilitar todo el ciclo de vida de un modelo de machine learning.

Por √∫ltimo se cuenta con una capa de frameworks e infraestructura, es una capa para los m√°s experimentados, en este nivel pr√°cticamente se tiene que programar desde cero todo el modelo, el despliegue corre igual por cuenta propia, da mas control pero pone mas estr√©s en el desarrollador.


El marco de trabajo OSEM en un estandard ampliamente usado para el modelado de datos en ciencia de datos, es especialmente √∫til en problemas de amplia escala.
OSEM establece actividades clara al momento de hablar de procesamiento de datos, las actividades son:
* Obtener (obtain)
* depurar (Scrub)
* Explorar (Explore)
* Modelar (Model)
* iNterpretar (iNterpret)

- Obtain:
Los datos se pueden generar a partir de ensayos cl√≠nicos, experimentos cient√≠ficos,
 encuestas, p√°ginas web, simulaciones por computadora, etc. 
Hay muchas formas en que se pueden almacenar los datos y parte del desaf√≠o inicial 
es simplemente leer los datos para poder analizarlos.
AWS cuenta con m√∫tiples opciones para compelar este paso, tal vez la mas vers√°lit es S3, 
aunque RDS, Redshift, Athena son opciones loables.

- Scrub:
La depuraci√≥n de datos se refiere al procesamiento previo necesario para preparar los datos antes del an√°lisis. Esto puede implicar eliminar filas o columnas particulares, 
manejar datos faltantes, corregir inconsistencias debido a errores de ingreso de datos, transformar fechas, generar variables derivadas o calculadas, combinar datos de m√∫ltiples fuentes, etc. 
Desafortunadamente no existe un m√©todo que pueda manejar todas las posibles necesidades de preprocesamiento de datos.

- Explore:
Una vez que sus datos est√©n listos para ser utilizados, y justo antes de pasar a la IA 
y al aprendizaje autom√°tico, tendr√° que explorar los datos.
En general, en un entorno corporativo o empresarial, el negocio simplemente 
arrojar√°n un conjunto de datos y depende de uno encontrarle sentido.
En primer lugar, se deber√° inspeccionar los datos y todas sus propiedades. 
Hay diferentes tipos de datos como datos num√©ricos, datos categ√≥ricos, datos ordinales 
y nominales, etc. Con eso, existen diferentes tipos de caracter√≠sticas de datos que requerir√°n que se manejen de manera diferente.
El t√©rmino "feature" que se utiliza en el aprendizaje autom√°tico o en el modelado, son las caracter√≠sticas de los datos para ayudarlo a identificar cu√°les son las caracter√≠sticas que representan los datos. 
Por ejemplo, "Nombre", "Edad", "Sexo" son caracter√≠sticas del conjunto de datos.

- Model:
Esta es la etapa m√°s interesante del ciclo de vida del proyecto de ciencia de datos. 
Como mucha gente lo llamar√≠a "donde ocurre la magia".

Una vez m√°s, antes de llegar a esta etapa, hay que tener en cuenta que la etapa de 
depuraci√≥n y exploraci√≥n es crucial para que este proceso tenga sentido.

Una de las primeras cosas que debe hacer al modelar datos es reducir la 
dimensionalidad de su conjunto de datos. No todas sus caracter√≠sticas o 
valores son esenciales para predecir su modelo. Por lo tanto, lo que se debe hacer es seleccionar los feature relevantes que contribuir√°n a la predicci√≥n de los resultados que est√° buscando.
Adem√°s de la clasificaci√≥n o predicci√≥n de los resultados, el prop√≥sito de esta etapa tambi√©n puede incluir la agrupaci√≥n de datos para comprender la l√≥gica detr√°s de esos grupos. 
Por ejemplo, si se requiere agrupar los clientes de un e-commerce para comprender 
su comportamiento en su sitio web se requerir√≠a que se identifiquen grupos de puntos de datos con algoritmos de agrupamiento como k-means; o hacer predicciones usando regresiones como regresiones lineales o log√≠sticas.

- iNterpret:
La interpretaci√≥n de datos se refiere b√°sicamente a la presentaci√≥n de los datos, 
entregando los resultados de tal manera que se puedan responder las preguntas que negocio hizo cuando comenz√≥ el proyecto por primera vez.
Se deber√°n visualizar los hallazgos  manteni√©ndolos impulsados por las preguntas de negocio. Es muy importante poder presentar los hallazgos de una manera que sea √∫til para la organizaci√≥n, o de lo contrario no tendr√≠a sentido para las partes interesadas.


Para comprender mejor los flujos de trabajo se explorar√° en Amazon SageMaker y Amazon Rekognition, hay muchos m√°s servicios implicados en temas de machine learnign, solo se tocan estos a modo introductorio ya que no entran de lleno en la certificaci√≥n pero vale la pena por la demanda en el mercado.


## Amazon Rekognition (AI Services):
El servicio de procesamiento de im√°genes de tipo plug and play, sin requerir dar mantenimiento y escalable bajo demanda, no se requiere ning√∫n tipo de experiencia con modelos de machine learning.
Tener el poder de identificar objetos, personas, texto en v√≠deos e im√°genes pr√°cticamente sin esfuerzo sin duda acelera mucho el proceso de desarrollo de aplicaciones y producto para el cliente final. Los precios van en funci√≥n del procesado por im√°gen o por minuto de v√≠deo. Rekognition cuenta con un √°mplio cat√°logo de funcionalidades:
* Detecci√≥n de objetos y escenas: √ötil para reconocer objetos en una escena como personas, bicicletas y monta√±as en una escena de bicicleta de monta√±a.
* Moderaci√≥n de contenido: ¬øse requiere la restricci√≥n o alerta ante contenido relacionado con juegs de azar, alcohol, cigarro drogas u otro contenido inapropiado? esta es la opci√≥n a elegir.
* An√°lisis facial: √ötil para extraer caracter√≠sticas del rostro de una persona tales como sexo, puntos de referencia del rostro,-
* Comparaci√≥n de rostros: Una aplicaci√≥n que requiera temas de seguridad por ejemplo evitar el acceso a un edificio por personas no gratas requerir√° que los rostros sean identificados y comparados con esa lista de rechazados.
* Detecci√≥n de texto: Da la capacidad a aplicaciones de tdetectar y reconocer texto de una imagen o video, en casetas de cobro puede ser una soluci√≥n √∫til si se requiere tomar la placa de los autos o en estacionamientos de edificios gubernamentales.
* EPP: Si se requieren cumplir normas estrictas de seguridad en el √°rea de trabajo esta es la opci√≥n id√≥nea.
![prework-epp-0001.png](prework-epp-0001.png)

B√°sicamente todos los tipos de procesamiento anterior aplican tanto para im√°genes como para v√≠deo.


## Amazon Textract (AI Services):
La migraci√≥n de datos de medios impresos (generalmente archivo) es una necesidad hoy d√≠a de bajo perfil. Tradicionalmente esta tarea se hace con un ej√©rcito de personas a modo de  capturistas de datos.
Con Textract el personal requerido puede aminorar, las capacidades de estraer texto de impresiones, texto de un documento a mano alzada, extracci√≥n de informaci√≥n de documentos y tablas.
El servicio es un servicio administrado de Amazon, no se requieren tareas de despliegue ni procesos de escalado ante altas demandas, el precio va en funci√≥n de las llamadas a la API que se hagan al servicio y el tipo de operaci√≥n requerida.

## Amazon Comprehend (AI Services):
Servicio administrado de Procesamiento de Lenguaje Natural, muy √∫til para extraer informaci√≥n de textos.
La informaci√≥n que puede extraerse es:
- Entities: Tales como fechas, n√∫meros telef√≥nicos, nombres de empresas, cantidades.
- An√°lisis de sentimiento: √ötil para conocer si el texto tiene connotaciones positivas o negativas.
- Sintaxis: Extrae informaci√≥n relacionada al tipo de palabras encontradas entre pronombres, sustantivos etc.
- Lenguaje: Identificaci√≥n del lenguaje de un texto.

## Amazon SageMaker (ML Services): 
Las tareas de generaci√≥n de modelos matem√°ticos basados en t√©cnicas de machine learning son retadores, normalmente requieren de  personal especializado, herramientas como Amazon SageMaker son herramientas que pueden ayudar a reducir la inclinaci√≥n de la pendiente de aprendizaje a niveles aceptables para comenzar a hacer prototipos r√°pidamente. 
SegeMaker es un entorno completo para la construcci√≥n,  entrenamiento y puesta en marcha de un modelo de machine learning. ¬øC√≥mo se logra?, con el uso de la conocida herramienta [Jupyter](https://jupyter.org/) ampliamente difundida entre personal especialista en temas de datos, es una interfaz web amigable para crear documentos con c√≥digo "vivo", ecuaciones y texo documental, con ella los procesos de depuraci√≥n de datos, simulaci√≥n, modelado y visualizaci√≥n de datos.
Todo eso es solo parte del proceso de construcci√≥n de un modelo de machine learning, es muy importante el set de datos con el que se entrenar√° y validar√° el modelo, hay m√∫ltiples set de datos en internet, aunque no siempre funcionan para los requerimientos espec√≠ficos, en caso de requerir un set de datos se puede hacer uso de Amazon SageMaker Ground Truth con ayuda de [crowdsourcing](https://dl.acm.org/doi/abs/10.1145/3318464.3383127) con Amazon Mechanical Turk o con un equipo o empresa de terceros. Tambi√©n es posible generar datasets de forma automatizada, Ground truth usa machine learning para decidir cuales datos deben ser etiquetados por personas, as√≠ el esfuerzo de labeling se aminora. Las t√©cnicas de generaci√≥n de datos  soportadas son [bounding box](https://keymakr.com/blog/what-are-bounding-boxes/), [segmentaci√≥n sem√°ntica](https://nanonets.com/blog/semantic-image-segmentation-2020/), [clasificaci√≥n de im√°genes](https://d2l.ai/chapter_linear-networks/image-classification-dataset.html), para texto se tiene [reconocimiento de entidades}https://monkeylearn.com/blog/named-entity-recognition/, [clasificaci√≥n de textos](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/), v√≠deo tambi√©n es soportado.

Para la parte de entrenamiento se cuentan con varios algoritmos BlazingText, DeepAR Forecasting, Image Classification Algorithm, Factorization Machines, K-Means Algorithm, K-Nearest Neighbors (k-NN) Algorithm, Random Cut Forest (RCF) Algorithm, BlazingText algorithm, Latent Dirichlet Allocation (LDA) Algorithm, Neural Topic Model (NTM) Algorithm por mencionar solo algunos.

Despu√©s de entrenar el modelo con el algoritmo adecuado se deber√° hacer el despliegue para que los usuarios finales lo puedan utilizar. Se puede generar un endpoint en un servicio hosteado en Segemaker ya sea un √∫nico modelo o m√∫ltiples modelos. Otra forma de hacerlo es por medio de batch, es una forma √∫til cuando se requieren obtener el resultado de m√∫ltiples datos en un dataset al mismo tiempo, ya que por medio del endpoint implicar√≠a varias llamadas, no ser√≠a un proceso muy √≥ptimo.

Ya en producci√≥n se debe proceder con un monitoreo para saber si el modelo se comporta como se esperaba o si el negocio ha cambiado y el modelo esta perdiendo precisi√≥n. Siempre la detecci√≥n de fallas tempranas es bien agradecido por los usuarios finales. Se tiene la posibilidad de usar un modelo de monitoreo predefinido o uno propietario.





